{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(Embed, self).__init__()\n",
    "        self.embed=nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \n",
    "        out=self.embed(inp)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pos_Embed(nn.Module):\n",
    "    def __init__(self, embed_size, max_size_input_seq):\n",
    "        super(Pos_Embed, self).__init__()\n",
    "        self.embed_size=embed_size\n",
    "\n",
    "        pos_em=torch.zeros(max_size_input_seq, embed_size)\n",
    "\n",
    "        for i in range(max_size_input_seq):\n",
    "            for j in range(embed_size//2):\n",
    "                temp=2*j/embed_size\n",
    "                pos_em[i][j]=math.sin(i/(10000**temp))\n",
    "                pos_em[i][j+1]=math.cos(i/(10000**temp))\n",
    "\n",
    "        self.pos_em=pos_em\n",
    "        \n",
    "\n",
    "    def forward(self, embeded_inp):\n",
    "\n",
    "        embeded_inp=embeded_inp\n",
    "\n",
    "        seq_len=embeded_inp.shape[1]\n",
    "\n",
    "        pos_embeded=embeded_inp+torch.autograd.Variable(self.pos_em[:seq_len,:])\n",
    "\n",
    "        return pos_embeded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, num_head, pos_embed_size):\n",
    "        super(Multi_Head_Attention, self).__init__()\n",
    "\n",
    "        self.num_head=num_head\n",
    "        self.pos_embed_size=pos_embed_size\n",
    "        self.each_head_pos_embed_size=pos_embed_size//num_head\n",
    "\n",
    "        self.q_mat_multiply=nn.Linear(self.pos_embed_size, self.pos_embed_size)\n",
    "        self.k_mat_multiply=nn.Linear(self.pos_embed_size, self.pos_embed_size)\n",
    "        self.v_mat_multiply=nn.Linear(self.pos_embed_size, self.pos_embed_size)\n",
    "\n",
    "        self.mult=nn.Linear(self.pos_embed_size, self.pos_embed_size)\n",
    "\n",
    "    def forward(self, key, query, value, mask=None):\n",
    "\n",
    "        batch_size=key.shape[0]\n",
    "        seq_len=key.shape[1]\n",
    "\n",
    "        query_seq_len=query.shape[1]\n",
    "\n",
    "        q_after_mult=self.q_mat_multiply(query)\n",
    "        k_after_mult=self.k_mat_multiply(key)\n",
    "        v_after_mult=self.v_mat_multiply(value)\n",
    "\n",
    "        key=k_after_mult.view(batch_size, seq_len, self.num_head, self.each_head_pos_embed_size)\n",
    "        query=q_after_mult.view(batch_size, query_seq_len, self.num_head, self.each_head_pos_embed_size)\n",
    "        value=v_after_mult.view(batch_size, seq_len, self.num_head, self.each_head_pos_embed_size)\n",
    "\n",
    "        prod=torch.einsum(\"nqhe,nlhe->nhql\",[query,key])\n",
    "\n",
    "        if mask is not None:\n",
    "            prod=prod.masked_fill(mask==0, float(\"-1e20\"))\n",
    "\n",
    "        prod=prod/math.sqrt(self.pos_embed_size)\n",
    "\n",
    "        att=torch.softmax(prod,dim=3)\n",
    "\n",
    "        final=torch.einsum(\"nhql,nlhe->nqhe\",[att,value])\n",
    "\n",
    "        final=final.reshape(batch_size,query_seq_len,self.pos_embed_size)\n",
    "\n",
    "        final_mult=self.mult(final)\n",
    "\n",
    "        return final_mult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm_and_MultiheadAttension(nn.Module):\n",
    "    def __init__(self, num_head, pos_embed_size, drop_out):\n",
    "        super(Norm_and_MultiheadAttension, self).__init__()\n",
    "        self.drop=drop_out\n",
    "\n",
    "        self.multiattention=Multi_Head_Attention(num_head, pos_embed_size)\n",
    "\n",
    "        self.norm=nn.LayerNorm(pos_embed_size)\n",
    "\n",
    "        self.drop_out=nn.Dropout(self.drop)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        output_after_attention=self.multiattention(key, query, value, mask)\n",
    "\n",
    "        output=output_after_attention+query\n",
    "\n",
    "        output=self.norm(output)\n",
    "\n",
    "        output=self.drop_out(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_and_Feed_forward(nn.Module):\n",
    "    def __init__(self, num_head, pos_embed_size, drop_out, expansion_factor):\n",
    "        super(Attention_and_Feed_forward, self).__init__()\n",
    "        self.drop=drop_out\n",
    "\n",
    "        self.attention_and_norm=Norm_and_MultiheadAttension(num_head, pos_embed_size,drop_out)\n",
    "\n",
    "        self.feed_forward=nn.Sequential(\n",
    "                                        nn.Linear(pos_embed_size, expansion_factor*pos_embed_size), \n",
    "                                        nn.ReLU(), \n",
    "                                        nn.Linear(expansion_factor*pos_embed_size, pos_embed_size), \n",
    "                                    )\n",
    "        \n",
    "        self.norm=nn.LayerNorm(pos_embed_size)\n",
    "\n",
    "        self.drop_out=nn.Dropout(self.drop)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        output_after_attention_norm=self.attention_and_norm(query, key, value, mask)\n",
    "\n",
    "        output_after_feed=self.feed_forward(output_after_attention_norm)\n",
    "\n",
    "        output=output_after_attention_norm+output_after_feed\n",
    "\n",
    "        output=self.norm(output)\n",
    "\n",
    "        output=self.drop_out(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_size_input_seq, num_of_layers_of_encoder, num_head, pos_embed_size, drop_out, expansion_factor):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embed=Embed(vocab_size, embed_size)\n",
    "\n",
    "        self.pos_embed=Pos_Embed(embed_size, max_size_input_seq)\n",
    "\n",
    "        self.num_of_layers_of_encoder=num_of_layers_of_encoder\n",
    "\n",
    "        self.layers=nn.ModuleList([Attention_and_Feed_forward(num_head, pos_embed_size, drop_out, expansion_factor) for i in range(num_of_layers_of_encoder)])\n",
    "\n",
    "        self.drop_out=nn.Dropout(drop_out)    \n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        embed_inp=self.embed(inp)\n",
    "\n",
    "        pos_embed_inp=self.pos_embed(embed_inp)\n",
    "\n",
    "        output=self.drop_out(pos_embed_inp)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output=layer(output, output, output, None)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Repeat(nn.Module):\n",
    "    def __init__(self, num_head, pos_embed_size, drop_out, expansion_factor):\n",
    "        super(Decoder_Repeat, self).__init__()\n",
    "\n",
    "        self.norm_masked_attention=Norm_and_MultiheadAttension(num_head, pos_embed_size, drop_out)\n",
    "\n",
    "        self.drop=drop_out\n",
    "\n",
    "        self.drop_out=nn.Dropout(self.drop)\n",
    "\n",
    "        self.attention_and_feed_forward=Attention_and_Feed_forward(num_head, pos_embed_size, drop_out, expansion_factor)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        after_masked_attension=self.norm_masked_attention(query, query, query, mask)\n",
    "\n",
    "        output=self.attention_and_feed_forward(after_masked_attension, key, value)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_size_input_seq, num_head, pos_embed_size, drop_out, expansion_factor, num_of_layers_of_decoder):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embed=Embed(vocab_size, embed_size)\n",
    "        \n",
    "        self.pos_embed=Pos_Embed(embed_size, max_size_input_seq)\n",
    "\n",
    "        self.layers=nn.ModuleList([Decoder_Repeat(num_head, pos_embed_size, drop_out, expansion_factor) for i in range(num_of_layers_of_decoder)])\n",
    "\n",
    "        self.linear_layer_last=nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "        self.drop_out=nn.Dropout(drop_out)\n",
    "\n",
    "    def forward(self, input_dec, encoder_output, mask):\n",
    "        inp_dec=self.embed(input_dec)\n",
    "\n",
    "        inp_dec=self.pos_embed(inp_dec)\n",
    "\n",
    "        value=self.drop_out(inp_dec)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            value=layer(value, encoder_output, encoder_output, mask)\n",
    "\n",
    "        output=self.linear_layer_last(value)\n",
    "\n",
    "        output=nn.functional.softmax(output,dim=2)\n",
    "\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_size_input_seq, num_of_layers_of_encoder, num_head, pos_embed_size, drop_out, expansion_factor, num_of_layers_of_decoder):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.max_seq_len=max_size_input_seq\n",
    "\n",
    "        self.encoder=Encoder(vocab_size, embed_size, max_size_input_seq, num_of_layers_of_encoder, num_head, pos_embed_size, drop_out, expansion_factor)\n",
    "        self.decoder=Decoder(vocab_size, embed_size, max_size_input_seq, num_head, pos_embed_size, drop_out, expansion_factor, num_of_layers_of_decoder)\n",
    "\n",
    "    def create_mask(self,trg):\n",
    "        batch_size,seq_len=trg.shape\n",
    "\n",
    "        mask=torch.tril(torch.ones((seq_len, seq_len))).expand(batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(self,src,trg):\n",
    "\n",
    "        mask_made=self.create_mask(trg)\n",
    "\n",
    "        encoder_output=self.encoder(src)\n",
    "\n",
    "        decoder_output=self.decoder(trg, encoder_output, mask_made)\n",
    "\n",
    "        return decoder_output\n",
    "    \n",
    "    def output(self,input):\n",
    "\n",
    "        input_str = np.array([max(1, min(ord(c) - ord('a') + 1, 27)) for c in input])\n",
    "        input = torch.tensor([input])\n",
    "\n",
    "        encoder_out=self.encoder(input)\n",
    "\n",
    "        batch_size=input.shape[0]\n",
    "\n",
    "        output=torch.zeros(batch_size,1)\n",
    "\n",
    "        remember=np.full((batch_size,1),0)\n",
    "\n",
    "        for _ in range(self.max_seq_len):\n",
    "            output=torch.tensor(remember)\n",
    "            target_mask=self.create_mask(output)\n",
    "            output=self.decoder(output,encoder_out,target_mask)\n",
    "            output=output[:,-1,:]\n",
    "            output=output.argmax(-1)\n",
    "            output = torch.unsqueeze(output,axis=1)\n",
    "            remember=np.concatenate((remember,output.numpy()),axis=1)\n",
    "            \n",
    "        return remember[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('./train_data.csv')\n",
    "eval_data=pd.read_csv('./eval_data.csv')\n",
    "\n",
    "train_input=train_data['Sentence']\n",
    "train_input=np.array([[ord(char) - ord('a') + 1  for char in string] for string in train_input])\n",
    "train_output=train_data['Transformed sentence']\n",
    "train_output=np.array([[ord(char) - ord('a') + 1 for char in string] for string in train_output])\n",
    "temp=np.zeros((7000,1))\n",
    "train_output=np.hstack((temp,train_output)).astype(np.int32)\n",
    "\n",
    "eval_input=eval_data['Sentence']\n",
    "eval_input=np.array([[ord(char) - ord('a') + 1 for char in string] for string in eval_input])\n",
    "eval_output=eval_data['Transformed sentence']\n",
    "eval_output=np.array([[ord(char) - ord('a') + 1 for char in string] for string in eval_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_train=DataLoader(train_input, batch_size=64, shuffle=False)\n",
    "out_train=DataLoader(train_output, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_size=27,embed_size=80,max_size_input_seq=8,num_of_layers_of_encoder=1,num_head=4,pos_embed_size=128,drop_out=0.3,expansion_factor=4,num_of_layers_of_decoder=1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses=[]\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss_epoch=[]\n",
    "\n",
    "    for inputs,targets in zip(inp_train,out_train):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs,targets[:,:-1])\n",
    "        temp_targets=torch.tensor(np.eye(27)[targets[:,1:]])\n",
    "        loss = criterion(outputs, temp_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "    print(\"Epoch : \",epoch+1,\"\\t Loss : \",sum(loss_epoch)/len(loss_epoch))\n",
    "\n",
    "    losses.append(sum(loss_epoch)/len(loss_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,input_str):\n",
    "    output = ''\n",
    "    input_str = np.array([ord(c) - ord('a') + 1 for c in input_str])\n",
    "    src = torch.tensor([input_str])\n",
    "    tgt = torch.tensor([[0]])\n",
    "    # print(src,tgt)\n",
    "    for _ in range(8):\n",
    "        pred = model(src, tgt)\n",
    "        pred = pred.argmax(dim=2)\n",
    "        print(pred)\n",
    "        output += chr(pred[0, -1] + ord('a') - 1)\n",
    "        tgt = torch.cat((tgt, pred[:, -1].unsqueeze(0)), dim=1)\n",
    "    return output\n",
    "\n",
    "print(predict(model, 'rgwuwrnh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check how many characters match in the two strings\n",
    "def check(pred: str, true: str):\n",
    "    correct = 0\n",
    "    for a, b in zip(pred, true):\n",
    "        if a == b:\n",
    "            correct += 1\n",
    "    return correct\n",
    "\n",
    "# Function to score the model's performance\n",
    "def evaluate(model):\n",
    "    print(\"Obtaining metrics for eval data:\")\n",
    "    eval_data = pd.read_csv(\"eval_data.csv\").to_numpy()\n",
    "    results = {\n",
    "        \"pred\": [],\n",
    "        \"true\": [],\n",
    "        \"score\": [],\n",
    "    }\n",
    "    correct = [0 for _ in range(9)]\n",
    "    for x, y in eval_data:\n",
    "        pred = predict(model, x)\n",
    "        print(f\"Predicted: {pred}, True: {y}\")\n",
    "        score = check(pred, y)\n",
    "        results[\"pred\"].append(pred)\n",
    "        results[\"true\"].append(y)\n",
    "        results[\"score\"].append(score)\n",
    "\n",
    "        correct[score] += 1\n",
    "    print(\"Eval dataset results:\")\n",
    "    for num_chr in range(9):\n",
    "        print(\n",
    "            f\"Number of predictions with {num_chr} correct predictions: {correct[num_chr]}\"\n",
    "        )\n",
    "    points = sum(correct[4:6]) * 0.5 + sum(correct[6:])\n",
    "    marks = round(min(2, points / 1400 * 2) * 2) / 2  # Rounds to the nearest 0.5\n",
    "    print(f\"Points: {points}\")\n",
    "    print(f\"Marks: {marks}\")\n",
    "    # Save predicitons and true sentences to inspect manually if required.\n",
    "    pd.DataFrame.from_dict(results).to_csv(\"results_eval_2.csv\", index=False)\n",
    "\n",
    "\n",
    "evaluate(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 11\n",
    "target_vocab_size = 11\n",
    "num_layers = 6\n",
    "seq_length= 12\n",
    "\n",
    "\n",
    "# let 0 be sos token and 1 be eos token\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1]])\n",
    "trg = torch.tensor([[0]])\n",
    "\n",
    "model = Transformer(src_vocab_size,embed_size=64,max_size_input_seq=12,num_of_layers_of_encoder=2,num_head=2,pos_embed_size=64,drop_out=0.3,expansion_factor=4,num_of_layers_of_decoder=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1], \n",
    "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "print(model.output(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output=model.encoder(src)\n",
    "\n",
    "output=[0]\n",
    "out=trg\n",
    "\n",
    "for i in range(seq_length-len(output)):\n",
    "    out=torch.tensor(output)\n",
    "    out=out.unsqueeze(0)\n",
    "    trg_mask=model.create_mask(out)\n",
    "    out=model.decoder(out,encoder_output,trg_mask)\n",
    "    out = out[:,-1,:]\n",
    "    out = out.argmax(-1)\n",
    "    out = torch.unsqueeze(out,axis=0)\n",
    "    output.append(out.item())\n",
    "\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
